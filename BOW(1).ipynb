{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fascinating\",\n",
        "    \"I enjoy learning about machine learning and deep learning\",\n",
        "    \"Language is a beautiful tool for communication\",\n",
        "    \"Machine learning algorithms are powerful\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"Data science combines statistics and computer science\",\n",
        "    \"I find NLP applications exciting\",\n",
        "    \"Artificial intelligence is transforming industries\",\n",
        "    \"Text processing involves a variety of techniques\",\n",
        "    \"Word embeddings capture semantic meaning\",\n",
        "    \"The field of AI is rapidly evolving\",\n",
        "    \"Chatbots use NLP to understand user queries\",\n",
        "    \"Sentiment analysis helps in understanding opinions\",\n",
        "    \"Speech recognition technology is improving\",\n",
        "    \"I often work with TensorFlow and Keras\",\n",
        "    \"Data visualization aids in data interpretation\",\n",
        "    \"Feature extraction is crucial in machine learning\",\n",
        "    \"Training data is essential for model performance\",\n",
        "    \"I read research papers on the latest ML techniques\",\n",
        "    \"Collaborative filtering is used in recommendation systems\"\n",
        "]\n",
        "\n",
        "# Data Preparation\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Generate Training Data (CBOW pairs)\n",
        "def generate_cbow_data(corpus, window_size=2):\n",
        "    input_words, target_words = [], []\n",
        "    for sentence in corpus:\n",
        "        tokenized = tokenizer.texts_to_sequences([sentence])[0]\n",
        "        for i in range(window_size, len(tokenized) - window_size):\n",
        "            context = [tokenized[j] for j in range(i - window_size, i + window_size + 1) if j != i]\n",
        "            input_words.append(context)\n",
        "            target_words.append(tokenized[i])\n",
        "    return np.array(input_words), np.array(target_words)\n",
        "\n",
        "X, y = generate_cbow_data(corpus)\n",
        "\n",
        "# Train the Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=10, input_length=X.shape[1]),\n",
        "    Flatten(),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Predict the target word from context\n",
        "def predict_word(context, window_size=2):\n",
        "    context_tokens = tokenizer.texts_to_sequences([context])[0]\n",
        "    context_tokens = [0] * (window_size * 2 - len(context_tokens)) + context_tokens if len(context_tokens) < window_size * 2 else context_tokens[-(window_size * 2):]\n",
        "    context_seq = np.array(context_tokens).reshape(1, -1)\n",
        "\n",
        "    predicted_index = np.argmax(model.predict(context_seq))\n",
        "    return tokenizer.index_word[predicted_index]\n",
        "\n",
        "# Test case\n",
        "test_context = \"no\"\n",
        "predicted_word = predict_word(test_context)\n",
        "print(f\"Predicted word for context '{test_context}': '{predicted_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7U6YEE3N57",
        "outputId": "82eba53d-799b-4be7-c689-36eec80b9841"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x795e717484c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "Predicted word for context 'no': 'a'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case\n",
        "test_context = \"I love natural language processing\"\n",
        "predicted_word = predict_word(test_context)\n",
        "print(f\"Predicted word for context '{test_context}': '{predicted_word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-QrlxQr8rks",
        "outputId": "ea5e91df-bee8-40cf-dfb1-92a20c91a739"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Predicted word for context 'I love natural language processing': 'natural'\n"
          ]
        }
      ]
    }
  ]
}